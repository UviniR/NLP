{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/uvinir/imdb-dataset-text-classification?scriptVersionId=113606355\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# **Text Classification Using Supervised Learning Algorithms**","metadata":{}},{"cell_type":"markdown","source":"This notebook will give you a breif idea on **Natural Language Processing** **using** **python** libraries.\n\nThe famous **imdb-datset** has been used here, which includes 50000 movie reviews classified as positive and negative.The aim of this study is to classify a new entry as either a positive review or a negative review.","metadata":{}},{"cell_type":"markdown","source":"### Install packages","metadata":{}},{"cell_type":"code","source":"pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:05:35.290561Z","iopub.execute_input":"2022-12-12T10:05:35.291012Z","iopub.status.idle":"2022-12-12T10:06:28.607587Z","shell.execute_reply.started":"2022-12-12T10:05:35.290973Z","shell.execute_reply":"2022-12-12T10:06:28.606035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import libraries","metadata":{}},{"cell_type":"code","source":"# pnadas to read data frames\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np \n\n# for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\n# for sql queries\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Row\nimport types\nfrom pyspark.sql.types import *\nfrom pyspark import SparkContext \nsc = SparkContext.getOrCreate() \nspark = SparkSession.builder.getOrCreate()\n\n# for NLP\nimport re #regular expressions\nimport nltk \nnltk.download('omw-1.4')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nstop_words = stopwords.words('english')\nimport string\n\n# for train test split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# Supervised learning algorithms  \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# For algorithm evalution\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:06:28.610257Z","iopub.execute_input":"2022-12-12T10:06:28.610657Z","iopub.status.idle":"2022-12-12T10:06:36.463245Z","shell.execute_reply.started":"2022-12-12T10:06:28.610606Z","shell.execute_reply":"2022-12-12T10:06:36.462016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read data file\nmovie_df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\nmovie_df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:06:36.464807Z","iopub.execute_input":"2022-12-12T10:06:36.465155Z","iopub.status.idle":"2022-12-12T10:06:38.090736Z","shell.execute_reply.started":"2022-12-12T10:06:36.465122Z","shell.execute_reply":"2022-12-12T10:06:38.089477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove duplicates\nmovie_df['dup'] = movie_df.duplicated(subset=None, keep='first')\ndel movie_df['dup']","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:06:38.095292Z","iopub.execute_input":"2022-12-12T10:06:38.096651Z","iopub.status.idle":"2022-12-12T10:06:38.310823Z","shell.execute_reply.started":"2022-12-12T10:06:38.096568Z","shell.execute_reply":"2022-12-12T10:06:38.309051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Class Balanced Data","metadata":{}},{"cell_type":"code","source":"# Applying SQL operations to create data frame\nclassNameContent = StructType([StructField(\"review\", StringType(), True),\n                               StructField(\"sentiment\",  StringType(), True)])\nFinalDataSet = spark.createDataFrame(movie_df, classNameContent)\nFinalDataSet.createTempView(\"MovieReviews\")\n\n# Check for class balanced nature\nprint(\"Total number of Reviews: \" + str(FinalDataSet.count()) )\nspark.sql(\n    \"select sentiment, count(sentiment) as count \" +\n    \"from MovieReviews \" +\n    \"group by sentiment \"\n    \"order by sentiment limit 20\" ).show()","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:06:38.312389Z","iopub.execute_input":"2022-12-12T10:06:38.31276Z","iopub.status.idle":"2022-12-12T10:06:48.824931Z","shell.execute_reply.started":"2022-12-12T10:06:38.312728Z","shell.execute_reply":"2022-12-12T10:06:48.823984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualise class blanace nature\nax = sns.countplot(x=\"sentiment\", data=movie_df)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:06:48.825869Z","iopub.execute_input":"2022-12-12T10:06:48.826173Z","iopub.status.idle":"2022-12-12T10:06:49.126838Z","shell.execute_reply.started":"2022-12-12T10:06:48.826145Z","shell.execute_reply":"2022-12-12T10:06:49.125642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pre Processing","metadata":{}},{"cell_type":"code","source":"# apply pre processing \n\nsw = stopwords.words('english') # call stopwords from nltk\nlemmatizer = WordNetLemmatizer() # call Lemmatisation from nltk\n\n# get a customised stopwords list\nstop_words_file = '/kaggle/input/smart-stop-list/SmartStoplist.txt' \nstop_words = []\nwith open(stop_words_file, \"r\") as f:\n    for line in f:\n        stop_words.extend(line.split())      \nstop_words = stop_words  \n\n# defining the preprocessing function\ndef preprocess(text):\n    \n    text = text.lower() #to convert into lowercase\n    \n    text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n\n    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n    \n    html=re.compile(r'<.*?>') \n    \n    text = html.sub(r'',text) #Removing html tags\n    \n    punctuations = '@#!?+&*[]-%.:/();$=><|{}^,' + \"'`\" + '_'\n    for p in punctuations:\n        text = text.replace(p,'') #Removing punctuations\n        \n    text = [word.lower() for word in text.split() if word.lower() not in sw] #removing stopwords\n    \n    \n    text = [lemmatizer.lemmatize(word) for word in text if lemmatizer.lemmatize(word) not in stop_words]\n    text = \" \".join(text) #Lemmatisation \n    \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    \n    text = emoji_pattern.sub(r'', text) #Removing emojis\n    \n    return text","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:06:49.128119Z","iopub.execute_input":"2022-12-12T10:06:49.128467Z","iopub.status.idle":"2022-12-12T10:06:49.148946Z","shell.execute_reply.started":"2022-12-12T10:06:49.128435Z","shell.execute_reply":"2022-12-12T10:06:49.147658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply preprocessing to review column\nmovie_df['prep'] = movie_df['review'].apply(lambda x: preprocess(x)) \ndel movie_df['review'] # remove review column\nmovie_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:06:49.150819Z","iopub.execute_input":"2022-12-12T10:06:49.151526Z","iopub.status.idle":"2022-12-12T10:09:22.766759Z","shell.execute_reply.started":"2022-12-12T10:06:49.151468Z","shell.execute_reply":"2022-12-12T10:09:22.765383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# observe a sample Review \nsample_corpora = movie_df['prep'].iloc[:1].values\nsample_corpora ","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:09:22.768402Z","iopub.execute_input":"2022-12-12T10:09:22.768903Z","iopub.status.idle":"2022-12-12T10:09:22.777254Z","shell.execute_reply.started":"2022-12-12T10:09:22.768861Z","shell.execute_reply":"2022-12-12T10:09:22.776097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenisation","metadata":{}},{"cell_type":"code","source":"# sql query to store the pre preprosessed data \nFinalDataSet=spark.createDataFrame(movie_df) \nFinalDataSet.printSchema()\nFinalDataSet.show(2)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:09:22.781259Z","iopub.execute_input":"2022-12-12T10:09:22.781688Z","iopub.status.idle":"2022-12-12T10:09:24.676934Z","shell.execute_reply.started":"2022-12-12T10:09:22.781649Z","shell.execute_reply":"2022-12-12T10:09:24.6757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert sentiment into binary values\nclasses=[\"negative\", \"positive\"]\nclassIx=[0,1]\nclassLookupMap=dict(zip(classes,classIx))","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:09:24.678219Z","iopub.execute_input":"2022-12-12T10:09:24.678793Z","iopub.status.idle":"2022-12-12T10:09:24.6859Z","shell.execute_reply.started":"2022-12-12T10:09:24.678748Z","shell.execute_reply":"2022-12-12T10:09:24.68452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize the content and convert the sentiment to a number\n# Convert content to array of words\nAllTokens_df = FinalDataSet.rdd.map(lambda text: Row(sentiment=classLookupMap[text[0]],prep=re.findall(r\"[\\w']+\" ,text[1].lower())) ).toDF()\n\nAllTokens_df.registerTempTable(\"allTokens\")\nAllTokens_df.printSchema()\nAllTokens_df.show(2)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:09:24.68731Z","iopub.execute_input":"2022-12-12T10:09:24.689477Z","iopub.status.idle":"2022-12-12T10:09:25.45604Z","shell.execute_reply.started":"2022-12-12T10:09:24.689425Z","shell.execute_reply":"2022-12-12T10:09:25.454656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split all the text files using non-Word characters \nAllTokensNonWordSplit = FinalDataSet.select('prep').rdd.flatMap(\n                        lambda text: re.findall(r\"[\\w']+\", text.prep.lower()) )\n\nprint(\"Number of tokens: \" + str(AllTokensNonWordSplit.count()) ) # all tokens\nprint(\"Number of distinct tokens: \" + str(AllTokensNonWordSplit.distinct().count()) ) # distinct tokens","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:09:25.45742Z","iopub.execute_input":"2022-12-12T10:09:25.457915Z","iopub.status.idle":"2022-12-12T10:09:32.213955Z","shell.execute_reply.started":"2022-12-12T10:09:25.457864Z","shell.execute_reply":"2022-12-12T10:09:32.212714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# spread of tokens per review\nspark.sql(\"\"\"\n  select min(sz) minimum, avg(sz) average, max(sz) maximum\n  from (\n    select size(prep) sz\n    from allTokens\n  )\n\"\"\").show()","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:09:32.215171Z","iopub.execute_input":"2022-12-12T10:09:32.215576Z","iopub.status.idle":"2022-12-12T10:09:35.759326Z","shell.execute_reply.started":"2022-12-12T10:09:32.215536Z","shell.execute_reply":"2022-12-12T10:09:35.757965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word Cloud","metadata":{}},{"cell_type":"code","source":"# Most common token\ntokens = StructType([StructField(\"token\",  StringType(), True)])\n\n# Create a dataframe\nAllTokens = spark.createDataFrame(\n                   AllTokensNonWordSplit.map(lambda x:[x]), tokens )\n\nAllTokens.registerTempTable(\"Tokens\")\n\nspark.sql(\"\"\"\n    select token, count(token) tokencount \n    from Tokens \n    group by token \n    order by tokencount desc \n    \"\"\").toPandas()","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:09:35.760714Z","iopub.execute_input":"2022-12-12T10:09:35.762233Z","iopub.status.idle":"2022-12-12T10:09:50.672135Z","shell.execute_reply.started":"2022-12-12T10:09:35.762184Z","shell.execute_reply":"2022-12-12T10:09:50.670743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In case if you need to save all the tokens use the below\n# I recommend you to have a look on individual tokens, update your stopword list and redo the above steps\n\n# AllTokensdf = AllTokens.toPandas()\n# AllTokensdf = AllTokensdf.groupby(\"token\")[\"token\"].count()\n# AllTokensdf.to_csv('/Users/uvini/Downloads/movietokens.csv')","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:09:50.673792Z","iopub.execute_input":"2022-12-12T10:09:50.67415Z","iopub.status.idle":"2022-12-12T10:09:50.679056Z","shell.execute_reply.started":"2022-12-12T10:09:50.67412Z","shell.execute_reply":"2022-12-12T10:09:50.677864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# produce wordclouds\n# Suggestion: we can genrate seperate word clouds for each category \n\nall_words = '' \n\n# to extract most common words\nfor arg in movie_df[\"prep\"]: \n\n    tokens = arg.split()  \n      \n    all_words += \" \".join(tokens)+\" \"\n\nwordcloud = WordCloud(width = 500, height = 400, \n                background_color ='white', \n                min_font_size = 10).generate(all_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (5, 5), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:09:50.680576Z","iopub.execute_input":"2022-12-12T10:09:50.681053Z","iopub.status.idle":"2022-12-12T10:10:41.672532Z","shell.execute_reply.started":"2022-12-12T10:09:50.681016Z","shell.execute_reply":"2022-12-12T10:10:41.671211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train-Test Split","metadata":{}},{"cell_type":"code","source":"X_train, X_test , y_train, y_test = train_test_split(movie_df['prep'].values,\n                                                     movie_df['sentiment'].values,test_size=0.2,\n                                                     random_state=42,stratify=movie_df['sentiment'].values)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:10:41.673979Z","iopub.execute_input":"2022-12-12T10:10:41.674425Z","iopub.status.idle":"2022-12-12T10:10:41.735819Z","shell.execute_reply.started":"2022-12-12T10:10:41.67438Z","shell.execute_reply":"2022-12-12T10:10:41.734544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF Vectorisation","metadata":{}},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer() \n\ntfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train)\n\ntfidf_test_vectors = tfidf_vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:10:41.737504Z","iopub.execute_input":"2022-12-12T10:10:41.737897Z","iopub.status.idle":"2022-12-12T10:10:47.479852Z","shell.execute_reply.started":"2022-12-12T10:10:41.737864Z","shell.execute_reply":"2022-12-12T10:10:47.478672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Supervised Learning Algorithms** ","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"classifier1 = RandomForestClassifier()\nclassifier1.fit(tfidf_train_vectors,y_train)\n\ny_pred1 = classifier1.predict(tfidf_test_vectors)\n\nprint(classification_report(y_test,y_pred1))\nprint(\"Accuracy score:\", accuracy_score(y_test,y_pred1))","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:10:47.481506Z","iopub.execute_input":"2022-12-12T10:10:47.4819Z","iopub.status.idle":"2022-12-12T10:14:21.332138Z","shell.execute_reply.started":"2022-12-12T10:10:47.481867Z","shell.execute_reply":"2022-12-12T10:14:21.33057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate confusion matrix\ncnf_matrix = confusion_matrix(y_test,y_pred1)\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:14:21.333843Z","iopub.execute_input":"2022-12-12T10:14:21.335235Z","iopub.status.idle":"2022-12-12T10:14:21.834585Z","shell.execute_reply.started":"2022-12-12T10:14:21.335186Z","shell.execute_reply":"2022-12-12T10:14:21.83332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Multinomial Naive Bayes","metadata":{}},{"cell_type":"code","source":"classifier3 = MultinomialNB()\nclassifier3.fit(tfidf_train_vectors,y_train)\n\ny_pred3 = classifier3.predict(tfidf_test_vectors)\n\nprint(classification_report(y_test,y_pred3))\nprint(\"Accuracy score:\", accuracy_score(y_test,y_pred3))","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:21:17.218281Z","iopub.execute_input":"2022-12-12T10:21:17.219262Z","iopub.status.idle":"2022-12-12T10:21:17.715738Z","shell.execute_reply.started":"2022-12-12T10:21:17.219216Z","shell.execute_reply":"2022-12-12T10:21:17.714021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"classifier4 = DecisionTreeClassifier()\nclassifier4.fit(tfidf_train_vectors,y_train)\n\ny_pred4 = classifier4.predict(tfidf_test_vectors)\n\nprint(classification_report(y_test,y_pred4))\nprint(\"Accuracy score:\", accuracy_score(y_test,y_pred4))","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:14:21.86021Z","iopub.execute_input":"2022-12-12T10:14:21.860665Z","iopub.status.idle":"2022-12-12T10:16:01.730204Z","shell.execute_reply.started":"2022-12-12T10:14:21.860602Z","shell.execute_reply":"2022-12-12T10:16:01.728972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K Neighbors Classifier","metadata":{}},{"cell_type":"code","source":"classifier5 = KNeighborsClassifier()\nclassifier5.fit(tfidf_train_vectors,y_train)\n\ny_pred5 = classifier5.predict(tfidf_test_vectors)\n\nprint(classification_report(y_test,y_pred5))\nprint(\"Accuracy score:\", accuracy_score(y_test,y_pred5))","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:16:01.731541Z","iopub.execute_input":"2022-12-12T10:16:01.732034Z","iopub.status.idle":"2022-12-12T10:16:37.038925Z","shell.execute_reply.started":"2022-12-12T10:16:01.731997Z","shell.execute_reply":"2022-12-12T10:16:37.037454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"classifier7 = LogisticRegression()\nclassifier7.fit(tfidf_train_vectors,y_train)\ny_pred7 = classifier7.predict(tfidf_test_vectors)\nprint(classification_report(y_test,y_pred7))\nprint(\"Accuracy score:\", accuracy_score(y_test,y_pred7))","metadata":{"execution":{"iopub.status.busy":"2022-12-12T10:17:16.826085Z","iopub.execute_input":"2022-12-12T10:17:16.826948Z","iopub.status.idle":"2022-12-12T10:17:25.903655Z","shell.execute_reply.started":"2022-12-12T10:17:16.826909Z","shell.execute_reply":"2022-12-12T10:17:25.90189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Additional Models to check out!!!**","metadata":{}},{"cell_type":"markdown","source":"### Ada Boost Classifier","metadata":{}},{"cell_type":"code","source":"classifier6 = AdaBoostClassifier()\nclassifier6.fit(tfidf_train_vectors,y_train)\n\ny_pred6 = classifier6.predict(tfidf_test_vectors)\n\nprint(classification_report(y_test,y_pred6))\nprint(\"Accuracy score:\", accuracy_score(y_test,y_pred6))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Support Vector Machine Classifier","metadata":{}},{"cell_type":"code","source":"classifier2 = SVC()\nclassifier2.fit(tfidf_train_vectors,y_train)\n\ny_pred2 = classifier2.predict(tfidf_test_vectors)\n\nprint(classification_report(y_test,y_pred2))\nprint(\"Accuracy score:\", accuracy_score(y_test,y_pred2))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### References","metadata":{}},{"cell_type":"markdown","source":"[https://github.com/jacquesroy/byte-size-data-science](http://)\n\n[https://github.com/dakshtrehan/Movie-Review-Classifier](http://)\n\n[https://github.com/NajiAboo/TextClassification](http://)","metadata":{}}]}